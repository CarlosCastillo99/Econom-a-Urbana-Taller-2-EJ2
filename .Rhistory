from   = x_min,
to     = x_max,
n      = 1024
)
# 4. Gráfico comparativo por año
par(mfrow = c(1, 1))
par(mar = c(5, 4, 4, 2) + 0.1)
# layout vertical
par(
mfrow = c(2, 1),          # 2 filas, 1 columna
mar   = c(3, 4, 2, 1)     # márgenes más pequeños para que nos quepa en el doc
)
# 2004
plot(
dens_2004_gauss,
lwd  = 2,
col  = "steelblue",
main = "Densidad no paramétrica de precios (2004)",
xlab = "Precio",
ylab = "Densidad estimada"
)
lines(
dens_2004_epan,
lwd = 2,
col = "firebrick",
lty = 2
)
legend(
"topright",
legend = c("Gaussiano", "Epanechnikov"),
col    = c("steelblue", "firebrick"),
lty    = c(1, 2),
lwd    = 2,
bty    = "n"
)
# 2012
plot(
dens_2012_gauss,
lwd  = 2,
col  = "steelblue",
main = "Densidad no paramétrica de precios (2012)",
xlab = "Precio",
ylab = "Densidad estimada"
)
lines(
dens_2012_epan,
lwd = 2,
col = "firebrick",
lty = 2
)
legend(
"topright",
legend = c("Gaussiano", "Epanechnikov"),
col    = c("steelblue", "firebrick"),
lty    = c(1, 2),
lwd    = 2,
bty    = "n"
)
# --------------------------------2.B--------------------------------------
# (b) KDE Epanechnikov con tres anchos de banda
# 1. Tres anchos de banda por año: h, h/2 y 2h
h_2004    <- bw_2004
h_2004_m  <- bw_2004 / 2      # mitad del rule-of-thumb
h_2004_2  <- 2 * bw_2004      # doble del rule-of-thumb
h_2012    <- bw_2012
h_2012_m  <- bw_2012 / 2
h_2012_2  <- 2 * bw_2012
cat("h_2004   =", h_2004,  "\n",
"h_2004/2 =", h_2004_m, "\n",
"2*h_2004 =", h_2004_2, "\n\n")
cat("h_2012   =", h_2012,  "\n",
"h_2012/2 =", h_2012_m, "\n",
"2*h_2012 =", h_2012_2, "\n")
# 2. Rango común de precios para todas las densidades
x_min <- min(c(prezzo_2004, prezzo_2012), na.rm = TRUE) - 5
x_max <- max(c(prezzo_2004, prezzo_2012), na.rm = TRUE) + 5
# 3. Densidades Epanechnikov 2004
dens_2004_epan_h   <- density(prezzo_2004, kernel = "epanechnikov",
bw = h_2004,   from = x_min, to = x_max, n = 1024)
dens_2004_epan_hm  <- density(prezzo_2004, kernel = "epanechnikov",
bw = h_2004_m, from = x_min, to = x_max, n = 1024)
dens_2004_epan_h2  <- density(prezzo_2004, kernel = "epanechnikov",
bw = h_2004_2, from = x_min, to = x_max, n = 1024)
# 4. Densidades Epanechnikov 2012
dens_2012_epan_h   <- density(prezzo_2012, kernel = "epanechnikov",
bw = h_2012,   from = x_min, to = x_max, n = 1024)
dens_2012_epan_hm  <- density(prezzo_2012, kernel = "epanechnikov",
bw = h_2012_m, from = x_min, to = x_max, n = 1024)
dens_2012_epan_h2  <- density(prezzo_2012, kernel = "epanechnikov",
bw = h_2012_2, from = x_min, to = x_max, n = 1024)
# rango comun eje Y
ylim_range <- range(
dens_2004_epan_h$y,  dens_2004_epan_hm$y,  dens_2004_epan_h2$y,
dens_2012_epan_h$y,  dens_2012_epan_hm$y,  dens_2012_epan_h2$y
)
# 5. Gráfico comparativo: 2004 y 2012 (tres bandas por año)
opar <- par(mfrow = c(2, 1),
mar   = c(4, 4, 3, 1) + 0.1)
## Panel superior: 2004
plot(dens_2004_epan_h,
lwd  = 2,
col  = "black",
main = "Precios 2004 (kernel Epanechnikov)",
xlab = "Precio",
ylab = "Densidad estimada",
ylim = ylim_range)   # <- aquí usas el rango común
lines(dens_2004_epan_hm, col = "steelblue",  lwd = 2, lty = 2)
lines(dens_2004_epan_h2, col = "firebrick",  lwd = 2, lty = 3)
legend("topright",
inset  = c(0.02,0.02),
legend = c("h (rule-of-thumb)", "h/2 (sub-suavizado)", "2h (sobre-suavizado)"),
col    = c("black", "steelblue", "firebrick"),
lty    = c(1, 2, 3),
lwd    = 2,
bty    = "n",
cex    = 0.8)
## Panel inferior: 2012
plot(dens_2012_epan_h,
lwd  = 2,
col  = "black",
main = "Precios 2012 (kernel Epanechnikov)",
xlab = "Precio",
ylab = "Densidad estimada",
ylim = ylim_range)   # <- mismo rango en el segundo panel
lines(dens_2012_epan_hm, col = "steelblue",  lwd = 2, lty = 2)
lines(dens_2012_epan_h2, col = "firebrick",  lwd = 2, lty = 3)
legend("topright",
inset  = c(0.02,0.02),
legend = c("h (rule-of-thumb)", "h/2 (sub-suavizado)", "2h (sobre-suavizado)"),
col    = c("black", "steelblue", "firebrick"),
lty    = c(1, 2, 3),
lwd    = 2,
bty    = "n",
cex    = 0.8)
par(opar)
##########################################################
# Universidad de los Andes
# Taller 2 ejericio 2
# Nombre: Carlos Castillo
##########################################################
rm(list=ls())
# cargamos paquetes
library(sf)
library(dplyr)
library(haven)
library(ggplot2)
library(scales)
#--------------------------- Punto 1---------------------------------------------------
# definimos ruta
setwd("C:/Users/pc/Downloads/taller 2 urbana ej2")
# 1. cargar bases y revisar----
# cargamos el shapefile
tracts_chi <- st_read("Boundaries - Census Tracts - 2010 (1)/geo_export_a4ade1ed-743c-4dd8-ad1a-89b46b222cec.shp") %>%
mutate(geoid10 = as.character(geoid10))  # asegurar que sea character
# Ver información básica
tracts_chi
names(tracts_chi)
head(tracts_chi)
# Veamos el mapa
plot(st_geometry(tracts_chi))
# Carguemos el panel con demografía por tract-año
panel <- read_dta("Combined_data_Panel (1).dta")
# Mira los nombres para ubicar ID y variables raciales:
names(panel)
# 2. Construcción de ID de tract y shares raciales (black / hispanos)----
panel <- panel %>%
mutate(
geoid10 = as.character(FIPS),            # ID del census tract en formato character (mismo que en el shapefile)
share_black    = Black_Pop    / Total_Pop,   # fracción de población afroamericana en el tract
share_hispanic = Hispanic_Pop / Total_Pop    # fracción de población hispana en el tract
)
#revisemos
panel                    # inspección rápida del panel completo
names(panel)             # nombres de las variables disponibles
head(panel)              # primeras filas para verificar que los shares se calcularon bien
# merge: panel demográfico + geometría
chi_panel <- panel %>%
left_join(tracts_chi, by = "geoid10") %>%  # unimos por geoid10 para añadir la geometría del tract
st_as_sf()                                 # convertimos el resultado a objeto sf
# Chequeos rápidos del merge
sum(is.na(chi_panel$geometry))  # cuántas observaciones quedaron sin geometría
table(chi_panel$year)           # número de tracts por año después del merge
# Miramos cómo quedó el objeto combinado
chi_panel
names(chi_panel)
head(chi_panel)
# Nos quedamos solo con observaciones que sí tienen geometría válida
chi_panel_sf <- chi_panel %>%
filter(!st_is_empty(geometry))
# Chequeos sobre el sf final
nrow(chi_panel_sf)                       # total de filas con geometría
sum(st_is_empty(chi_panel_sf$geometry))  # debería ser 0
table(chi_panel_sf$year)                 # distribución por año en el objeto final
# Revisión final del objeto que usaremos para mapas y regresiones
chi_panel_sf
names(chi_panel_sf)
head(chi_panel_sf)
# 3.1 Mapas para 2000: % población afroamericana por tract----
# Filtramos solo año 2000 y con dato de share_black
chi_black_2000 <- chi_panel_sf %>%
filter(year == 2000, !is.na(share_black))
#revisemos
chi_black_2000
names(chi_black_2000)
head(chi_black_2000)
# Mapa afroamericanos, 2000
ggplot(chi_black_2000) +
geom_sf(aes(fill = share_black),
color = "grey30",  # bordes de los tracts
size  = 0.1) +     # grosor de la línea
coord_sf(datum = NA) +    # quita el grid de lat/long
scale_fill_viridis_c(
option   = "magma",
direction = -1,
limits   = c(0, 1),
labels   = percent_format(accuracy = 1),
name     = "% afroamericanos"
) +
labs(
title = "Chicago – % población afroamericana por census tract (2000)",
x = NULL, y = NULL
) +
theme_void() +            # sin ejes ni fondo
theme(
plot.title = element_text(hjust = 0.5)
)
# 3.2 Mapas para 2000: % población hispana por tract----
# Ahora filtramos hispanos año 2000
chi_hisp_2000 <- chi_panel_sf %>%
filter(year == 2000, !is.na(share_hispanic))
# Mapa hispanos, 2000
ggplot(chi_hisp_2000) +
geom_sf(aes(fill = share_hispanic),
color = "grey30",
size  = 0.1) +
coord_sf(datum = NA) +
scale_fill_viridis_c(
option   = "magma",
direction = -1,
limits   = c(0, 1),
labels   = scales::percent_format(accuracy = 1),
name     = "% hispanos"
) +
labs(
title = "Chicago – % población hispana por census tract (2000)",
x = NULL, y = NULL
) +
theme_void() +
theme(
plot.title = element_text(hjust = 0.5)
)
# 3.3Ahora hagamos uno conjunto para afroamericanos e hispanos----
# 3.3.a Mapa afroamericanos para 2000,2015 y 2020----
chi_black_all <- chi_panel_sf %>%
filter(!is.na(share_black))
ggplot(chi_black_all) +
geom_sf(aes(fill = share_black),
color = "grey30",
size  = 0.1) +
coord_sf(datum = NA) +
scale_fill_viridis_c(
option    = "magma",
direction = -1,
limits    = c(0, 1),
labels    = scales::percent_format(accuracy = 1),
name      = "% afroamericanos"
) +
facet_wrap(~ year) +
labs(
title = "Chicago – % población afroamericana por census tract",
x = NULL, y = NULL
) +
theme_void() +
theme(
plot.title = element_text(hjust = 0.5)
)
# 3.3.b Mapa hispanos para 2000,2015 y 2020----
chi_hisp_all <- chi_panel_sf %>%
filter(!is.na(share_hispanic))
ggplot(chi_hisp_all) +
geom_sf(aes(fill = share_hispanic),
color = "grey30",
size  = 0.1) +
coord_sf(datum = NA) +
scale_fill_viridis_c(
option    = "magma",      # misma paleta que black
direction = -1,
limits    = c(0, 1),
labels    = scales::percent_format(accuracy = 1),
name      = "% hispanos"
) +
facet_wrap(~ year) +
labs(
title = "Chicago – % población hispana por census tract",
x = NULL, y = NULL
) +
theme_void() +
theme(
plot.title = element_text(hjust = 0.5)
)
# 4. Correlaciones con el ingreso mediano del census tract----
# 4.1 Correlación share_black vs ingreso----
corr_black <- chi_panel_sf %>%
st_drop_geometry() %>%
filter(!is.na(share_black), !is.na(Median_Inc)) %>%
group_by(year) %>%
summarise(
cor_black_inc = cor(share_black, Median_Inc, use = "complete.obs"),
.groups = "drop"
)
corr_black
names(corr_black)
head(corr_black)
# Afroamericanos vs ingreso por tract y año
ggplot(
chi_panel_sf %>% filter(!is.na(share_black), !is.na(Median_Inc)),
aes(x = share_black, y = Median_Inc)
) +
geom_point(alpha = 0.4, size = 1) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
facet_wrap(~ year) +
scale_x_continuous(labels = percent_format(accuracy = 1),
name   = "% afroamericanos en el tract") +
scale_y_continuous(labels = comma,
name   = "Ingreso mediano del census tract (USD)") +
labs(title = "Relación entre % afroamericanos e ingreso mediano por census tract") +
theme_minimal()
# 4.2 Correlación share_hispanic vs ingreso----
corr_hisp <- chi_panel_sf %>%
st_drop_geometry() %>%
filter(!is.na(share_hispanic), !is.na(Median_Inc)) %>%
group_by(year) %>%
summarise(
cor_hisp_inc = cor(share_hispanic, Median_Inc, use = "complete.obs"),
.groups = "drop"
)
corr_hisp
names(corr_hisp)
head(corr_hisp)
#Hispanos vs ingreso por tract y año
ggplot(
chi_panel_sf %>% filter(!is.na(share_hispanic), !is.na(Median_Inc)),
aes(x = share_hispanic, y = Median_Inc)
) +
geom_point(alpha = 0.4, size = 1) +
geom_smooth(method = "lm", se = FALSE, color = "red") +
facet_wrap(~ year) +
scale_x_continuous(labels = percent_format(accuracy = 1),
name   = "% hispanos en el tract") +
scale_y_continuous(labels = comma,
name   = "Ingreso mediano del census tract (USD)") +
labs(title = "Relación entre % hispanos e ingreso mediano por census tract") +
theme_minimal()
# Función de poder analítico para diferencia de proporciones (dos grupos independientes)
power_diff_prop <- function(p1, p2, n1, n2 = n1, alpha = 0.05) {
delta   <- p2 - p1
var_hat <- p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2
se_hat  <- sqrt(var_hat)
z_alpha <- qnorm(1 - alpha / 2)    # test bilateral
z_eff   <- abs(delta) / se_hat     # tamaño de efecto en unidades-z
# Aproximación habitual: Z ~ N(z_eff, 1)
power   <- pnorm(z_eff - z_alpha)
return(power)
}
# Ejemplo: poder para detectar una diferencia de -0.15 (0.40 vs 0.25)
p_B <- 0.40  # Björn (hombre sueco)
p_M <- 0.25  # Muhammad (hombre árabe)
Js <- c(500, 750, 1000, 1500)  # número de anuncios
power_vals <- sapply(Js, function(J) power_diff_prop(p1 = p_B, p2 = p_M, n1 = J, n2 = J))
data.frame(J = Js, power_analitico = round(power_vals, 3))
library(sandwich)
library(lmtest)
# Función de poder analítico para diferencia de proporciones (dos grupos independientes)
power_diff_prop <- function(p1, p2, n1, n2 = n1, alpha = 0.05) {
delta   <- p2 - p1
var_hat <- p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2
se_hat  <- sqrt(var_hat)
z_alpha <- qnorm(1 - alpha / 2)    # test bilateral
z_eff   <- abs(delta) / se_hat     # tamaño de efecto en unidades-z
# Aproximación habitual: Z ~ N(z_eff, 1)
power   <- pnorm(z_eff - z_alpha)
return(power)
}
# Ejemplo: poder para detectar una diferencia de -0.15 (0.40 vs 0.25)
p_B <- 0.40  # Björn (hombre sueco)
p_M <- 0.25  # Muhammad (hombre árabe)
Js <- c(500, 750, 1000, 1500)  # número de anuncios
power_vals <- sapply(Js, function(J) power_diff_prop(p1 = p_B, p2 = p_M, n1 = J, n2 = J))
data.frame(J = Js, power_analitico = round(power_vals, 3))
install.packages(c("sandwich", "lmtest"))
library(sandwich)
library(lmtest)
# Función de poder analítico para diferencia de proporciones (dos grupos independientes)
power_diff_prop <- function(p1, p2, n1, n2 = n1, alpha = 0.05) {
delta   <- p2 - p1
var_hat <- p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2
se_hat  <- sqrt(var_hat)
z_alpha <- qnorm(1 - alpha / 2)    # test bilateral
z_eff   <- abs(delta) / se_hat     # tamaño de efecto en unidades-z
# Aproximación habitual: Z ~ N(z_eff, 1)
power   <- pnorm(z_eff - z_alpha)
return(power)
}
# Ejemplo: poder para detectar una diferencia de -0.15 (0.40 vs 0.25)
p_B <- 0.40  # Björn (hombre sueco)
p_M <- 0.25  # Muhammad (hombre árabe)
Js <- c(500, 750, 1000, 1500)  # número de anuncios
power_vals <- sapply(Js, function(J) power_diff_prop(p1 = p_B, p2 = p_M, n1 = J, n2 = J))
data.frame(J = Js, power_analitico = round(power_vals, 3))
install.packages(c("sandwich", "lmtest"))
library(sandwich)
library(lmtest)
# Una simulación para un J dado
simulate_once <- function(J, p_B = 0.40, p_A = 0.45, p_M = 0.25) {
# 1) Construir data frame con 3 solicitantes por anuncio
listing_id <- 1:J
df <- expand.grid(
listing = listing_id,
type    = c("B", "A", "M"),  # Björn, Astrid, Muhammad
KEEP.OUT.ATTRS = FALSE,
stringsAsFactors = FALSE
)
# Función de poder analítico para diferencia de proporciones (dos grupos independientes)
power_diff_prop <- function(p1, p2, n1, n2 = n1, alpha = 0.05) {
delta   <- p2 - p1
var_hat <- p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2
se_hat  <- sqrt(var_hat)
z_alpha <- qnorm(1 - alpha / 2)    # test bilateral
z_eff   <- abs(delta) / se_hat     # tamaño de efecto en unidades-z
# Aproximación habitual: Z ~ N(z_eff, 1)
power   <- pnorm(z_eff - z_alpha)
return(power)
}
# Ejemplo: poder para detectar una diferencia de -0.15 (0.40 vs 0.25)
p_B <- 0.40  # Björn (hombre sueco)
p_M <- 0.25  # Muhammad (hombre árabe)
Js <- c(500, 750, 1000, 1500)  # número de anuncios
power_vals <- sapply(Js, function(J) power_diff_prop(p1 = p_B, p2 = p_M, n1 = J, n2 = J))
data.frame(J = Js, power_analitico = round(power_vals, 3))
install.packages(c("sandwich", "lmtest"))
library(sandwich)
library(lmtest)
# Una simulación para un J dado
simulate_once <- function(J, p_B = 0.40, p_A = 0.45, p_M = 0.25) {
# 1) Construir data frame con 3 solicitantes por anuncio
listing_id <- 1:J
df <- expand.grid(
listing = listing_id,
type    = c("B", "A", "M"),  # Björn, Astrid, Muhammad
KEEP.OUT.ATTRS = FALSE,
stringsAsFactors = FALSE
)
# 2) Asignar probabilidades según tipo
df$p <- ifelse(df$type == "B", p_B,
ifelse(df$type == "A", p_A, p_M))
# 3) Simular outcome de callback
df$Y <- rbinom(n = nrow(df), size = 1, prob = df$p)
# 4) Variables de tratamiento (base: Björn)
df$A <- as.integer(df$type == "A")  # Astrid (mujer sueca)
df$M <- as.integer(df$type == "M")  # Muhammad (hombre árabe)
# 5) Modelo de probabilidad lineal con cluster a nivel anuncio
mod <- lm(Y ~ A + M, data = df)
vcov_cl <- vcovCL(mod, cluster = ~ listing)  # clustering por anuncio
test_M <- coeftest(mod, vcov. = vcov_cl)["M", ]  # efecto Muhammad vs Björn
p_value_M <- test_M["Pr(>|t|)"]
return(p_value_M)
}
# Función de poder analítico para diferencia de proporciones (dos grupos independientes)
power_diff_prop <- function(p1, p2, n1, n2 = n1, alpha = 0.05) {
delta   <- p2 - p1
var_hat <- p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2
se_hat  <- sqrt(var_hat)
z_alpha <- qnorm(1 - alpha / 2)    # test bilateral
z_eff   <- abs(delta) / se_hat     # tamaño de efecto en unidades-z
# Aproximación habitual: Z ~ N(z_eff, 1)
power   <- pnorm(z_eff - z_alpha)
return(power)
}
# Ejemplo: poder para detectar una diferencia de -0.15 (0.40 vs 0.25)
p_B <- 0.40  # Björn (hombre sueco)
p_M <- 0.25  # Muhammad (hombre árabe)
Js <- c(500, 750, 1000, 1500)  # número de anuncios
power_vals <- sapply(Js, function(J) power_diff_prop(p1 = p_B, p2 = p_M, n1 = J, n2 = J))
data.frame(J = Js, power_analitico = round(power_vals, 3))
install.packages(c("sandwich", "lmtest"))
library(sandwich)
library(lmtest)
# Una simulación para un J dado
simulate_once <- function(J, p_B = 0.40, p_A = 0.45, p_M = 0.25) {
# 1) Construir data frame con 3 solicitantes por anuncio
listing_id <- 1:J
df <- expand.grid(
listing = listing_id,
type    = c("B", "A", "M"),  # Björn, Astrid, Muhammad
KEEP.OUT.ATTRS = FALSE,
stringsAsFactors = FALSE
)
# 2) Asignar probabilidades según tipo
df$p <- ifelse(df$type == "B", p_B,
ifelse(df$type == "A", p_A, p_M))
# 3) Simular outcome de callback
df$Y <- rbinom(n = nrow(df), size = 1, prob = df$p)
# 4) Variables de tratamiento (base: Björn)
df$A <- as.integer(df$type == "A")  # Astrid (mujer sueca)
df$M <- as.integer(df$type == "M")  # Muhammad (hombre árabe)
# 5) Modelo de probabilidad lineal con cluster a nivel anuncio
mod <- lm(Y ~ A + M, data = df)
vcov_cl <- vcovCL(mod, cluster = ~ listing)  # clustering por anuncio
test_M <- coeftest(mod, vcov. = vcov_cl)["M", ]  # efecto Muhammad vs Björn
p_value_M <- test_M["Pr(>|t|)"]
return(p_value_M)
}
